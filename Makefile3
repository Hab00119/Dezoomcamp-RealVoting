.PHONY: up-postgres up-bigquery down start-infra start-processing-postgres start-processing-bigquery start-apps start-monitoring logs clean setup-gcp start-streaming start-batch start-transform start-dashboard start-orchestration start-all-postgres start-all-bigquery

# Start everything with PostgreSQL in the correct order with proper delays
up-postgres:
	@echo "Starting infrastructure (ZooKeeper, Kafka, PostgreSQL)..."
	docker-compose --profile postgres up -d zookeeper kafka postgres
	@echo "Waiting for infrastructure to initialize (20 seconds)..."
	@sleep 20
	
	@echo "Starting monitoring tools (pgweb)..."
	docker-compose --profile postgres up -d pgweb

	@echo "Starting data generator..."
	docker-compose up -d data-generator
	@echo "Waiting for data generation (5 seconds)..."
	@sleep 5
	
	@echo "Starting vote simulator..."
	docker-compose up -d vote-simulator
	
	@echo "Starting processing services (dlt-pipeline-postgres)..."
	docker-compose --profile postgres up -d dlt-pipeline-postgres
	@echo "Waiting for processing services (10 seconds)..."
	@sleep 10
	
	@echo "All services started successfully!"
	@echo "Run 'make logs' to see the logs"

# Start everything with BigQuery in the correct order with proper delays
up-bigquery:
	@echo "Starting infrastructure (ZooKeeper, Kafka)..."
	docker-compose --profile bigquery up -d zookeeper kafka
	@echo "Waiting for infrastructure to initialize (20 seconds)..."
	@sleep 20

	@echo "Starting data generator..."
	docker-compose up -d data-generator
	@echo "Waiting for data generation (5 seconds)..."
	@sleep 5
	
	@echo "Starting vote simulator..."
	docker-compose up -d vote-simulator
	
	@echo "Starting processing services (dlt-pipeline-bigquery)..."
	docker-compose --profile bigquery up -d dlt-pipeline-bigquery
	@echo "Waiting for processing services (10 seconds)..."
	@sleep 10
	
	@echo "All services started successfully!"
	@echo "Run 'make logs' to see the logs"

# Start a complete PostgreSQL pipeline including new components
start-all-postgres: up-postgres start-streaming start-batch start-transform start-dashboard start-orchestration
	@echo "Complete PostgreSQL pipeline started!"

# Start a complete BigQuery pipeline including new components
start-all-bigquery: up-bigquery start-streaming start-batch start-transform start-dashboard start-orchestration
	@echo "Complete BigQuery pipeline started!"

# Start only infrastructure components
start-infra:
	@echo "Starting ZooKeeper and Kafka..."
	docker-compose up -d zookeeper kafka
	@echo "Waiting for infrastructure to initialize (20 seconds)..."
	@sleep 20
	@echo "Infrastructure is ready!"

# Start PostgreSQL if needed
start-postgres:
	@echo "Starting PostgreSQL..."
	docker-compose --profile postgres up -d postgres
	@echo "Waiting for PostgreSQL to initialize (10 seconds)..."
	@sleep 10
	@echo "PostgreSQL is ready!"

# Start only PostgreSQL processing component
start-processing-postgres:
	@echo "Starting DLT pipeline for PostgreSQL..."
	docker-compose --profile postgres up -d dlt-pipeline-postgres
	@echo "Processing service started successfully!"

# Start only BigQuery processing component
start-processing-bigquery:
	@echo "Starting DLT pipeline for BigQuery..."
	docker-compose --profile bigquery up -d dlt-pipeline-bigquery
	@echo "Processing service started successfully!"

# Start only application components
start-apps:
	@echo "Starting data generator..."
	docker-compose up -d data-generator
	@echo "Waiting for data generation (5 seconds)..."
	@sleep 5
	@echo "Starting vote simulator..."
	docker-compose up -d vote-simulator
	@echo "Applications started successfully!"

# Start only monitoring tools
start-monitoring:
	@echo "Starting monitoring tools (pgweb)..."
	docker-compose --profile postgres up -d pgweb
	@echo "Monitoring tools started successfully!"

# Start PyFlink streaming processing
start-streaming:
	@echo "Starting PyFlink streaming processor..."
	docker-compose --profile streaming up -d flink-processor
	@echo "Streaming processing started successfully!"

# Start PySpark batch processing
start-batch:
	@echo "Starting PySpark batch processor..."
	docker-compose --profile batch up -d spark-processor
	@echo "Batch processing started successfully!"

# Start dbt transformations
start-transform:
	@echo "Starting dbt transformations..."
	docker-compose --profile transform up -d dbt
	@echo "Transformations started successfully!"

# Start Streamlit dashboard
start-dashboard:
	@echo "Starting Streamlit dashboard..."
	docker-compose --profile dashboard up -d streamlit
	@echo "Dashboard started successfully at http://localhost:8501"

# Start Kestra orchestration
start-orchestration:
	@echo "Starting Kestra orchestration..."
	docker-compose --profile orchestration up -d kestra
	@echo "Orchestration started successfully at http://localhost:8080"

# Show logs for all services
logs:
	docker-compose logs -f

# Show logs for specific service group
logs-infra:
	docker-compose logs -f zookeeper kafka

logs-postgres:
	docker-compose logs -f postgres dlt-pipeline-postgres

logs-bigquery:
	docker-compose logs -f dlt-pipeline-bigquery

logs-apps:
	docker-compose logs -f data-generator vote-simulator

logs-streaming:
	docker-compose logs -f flink-processor

logs-batch:
	docker-compose logs -f spark-processor

logs-transform:
	docker-compose logs -f dbt

logs-dashboard:
	docker-compose logs -f streamlit

logs-orchestration:
	docker-compose logs -f kestra

logs-monitoring:
	docker-compose logs -f pgweb

# Stop and remove all containers
down:
	docker-compose down

# Remove volumes and any created data
clean:
	docker-compose down -v
	@echo "Cleaned up all containers and volumes"

# Default parameters for GCP setup
PROJECT_ID ?= 
CREDENTIALS_FILE ?= 
REGION ?= us-central1
DATASET_ID ?= voting_data
ENVIRONMENT ?= dev

# Setup GCP resources using Terraform and the setup script
setup-gcp:
	@if [ -z "$(PROJECT_ID)" ]; then \
		echo "ERROR: PROJECT_ID is required. Usage: make setup-gcp PROJECT_ID=your-project-id CREDENTIALS_FILE=path/to/credentials.json"; \
		exit 1; \
	fi
	@if [ -z "$(CREDENTIALS_FILE)" ]; then \
		echo "ERROR: CREDENTIALS_FILE is required. Usage: make setup-gcp PROJECT_ID=your-project-id CREDENTIALS_FILE=path/to/credentials.json"; \
		exit 1; \
	fi
	@echo "Setting up GCP resources with Terraform..."
	@chmod +x scripts/setup_gcp.sh
	@./scripts/setup_gcp.sh --project-id $(PROJECT_ID) \
		--credentials $(CREDENTIALS_FILE) \
		--region $(REGION) \
		--dataset-id $(DATASET_ID) \
		--environment $(ENVIRONMENT)

# One-command setup and run with BigQuery
bigquery-full-setup: setup-gcp up-bigquery
	@echo "BigQuery setup complete and services running"